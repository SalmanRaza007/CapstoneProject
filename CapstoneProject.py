# -*- coding: utf-8 -*-
"""CapstoneProject_SyedSalmanRazaNaqvi

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M-Q_XGp-oLmvceaROgAloCl7uOEaywyz

# **CAPSTONE PROJECT BY SYED SALMAN RAZA NAQVI**



---



### Capstone project submitted to International Business School for the partial fulfillment of the requirement for the degree of MASTER OF SCIENCE IN IT FOR BUSINESS DATA ANALYTICS



---



#### December 2021

# **Introduction**



---

This Coding Project aka Capstone Project at IBS is a major piece of written work that concludes our study program in pursuit of the graduate degree. The purpose of this project is to test our abilities to resolve problems in the practical world pertaining to data analysis by use of data science tools. The final product of this exercise aims to work on relevant codes and to provide an explanation for deeper insights into how we will resolve the problem at hand.

The project aims to follow an evidence backed decision making trajectory while it attempts to implement good literate programming and data visualizations where needed for conveying our results.

## **Abstract**


---

Air particulate pollution is a major source of environmental problem faced by developing economies, especially a rapidly developing one such as China. It is not only a major source of concern for the general health of the population but also stands as a big hurdle for economic production.

In our study, we will build a PM2.5 Concentration forcast model based on Long-Short Term Memory (LSTM) neural network method and evaluate its performance of predicting PM2.5 daily concentrations in Beijing, China.

From our initial findings we found that there is a positive correlation between pollution levels (PM2.5) and Dew Point as well as Snow. This makes sense as with both dew and snow means there are more particals in the atmosphere. All other input variables however have a negative correlation with PM2.5. For example, when it rains or when the temperature is higher, we can expect less air particulates in the atmosphere.

The complete columns of the dataset are the following:



*   No: row number
*   year: year of data in this row
*   month: month of data in this row
*   day: day of data in this row
*   hour: hour of data in this row
*   pm2.5: PM2.5 concentration
*   DEWP: Dew Point
*   TEMP: Temperature
*   PRES: Pressure
*   cbwb: Combined wind direction
*   Iws: Cumulated wind speed
*   Is: Cumulated hours of snow
*   Ir: Cumulated hours of rain

## **Importing libraries**


---



Python libraries are a very useful set of functions that eliminate the need to writing long lines of codes from scratch. They play a pivotal role in developing our data for machine learning and data visualization, both key aspects in our project. We will be using the following libraries with a brief info on their use:



---



*   ***NumPy***: NumPy or 'Numeric Python' is a powerful and core library for scientific computation. It adds powerful data structures to Python which help in efficient calculations with arrays and matrices. It's main data structure is the high-performance multidimensional array (ndarray) object. In essence, NumPy guarantees fast mathematical operations.



---



*   ***Pandas***: Pandas is another open source Python library with data manipulation tools which is built on top of NumPy. Hence, it is built upon the same logic and functionalities however, it also includes spreadsheet-like features. The spreadsheet-like 2D arrays are know as a DataFrame.



---


*   ***Seaborn***: Seaborn is an open source library built on Matplotlib. It is used for data visualization and exploratory data analysis.



---




*   ***Matplotlib***: Matplotlib is a data visualization and graphical plotting library used in Pandas and its numerical extension NumPy. It allows us visual access to large amounts of data that are easily digestible. Matplotlib consists of several plots including line, bar, scatter, histogram, etc.



---




*   ***Scikit-Learn (sklearn)***: The sklearn library contains many tools for machine learning and statistical modeling. It helps set up the data, split the data into training and test sets, tune the model and refit the training set, evaluate the model on the test set and save the model for further use.



---





*   ***TensorFlow***: TensorFlow is an open source library that allows developers to build neural networks with many layers. TensorFlow plays a major role in text-based applications, image recognition, voice search etc.



---




*   ***Keras***: Keras is an open source Neural Network library that runs on top of TensorFlow. It is designed to be fast and easy to use. It is built in Python which makes it easier to use.
"""

# importing all the required libraries

from pandas import read_csv
from datetime import datetime
import seaborn
import matplotlib.pyplot as plt
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import LabelEncoder
from math import sqrt
from numpy import concatenate
from pandas import concat
from pandas import DataFrame
from matplotlib import pyplot
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

"""## **Data Downloading**


---


"""

!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv

"""## **Data Preperation**


---

 

*   Step 1: We will parse through the year, month, day and hour columns and consolidate them to set our datetime column as index. We will then drop the unnecessary columns.
*   Step 2: We will assign new names to each column.
*   Step 3: Dealing with NaN values. We fill the NaN values with 0 and drop the first  24 rows.
*   Step 4: Saving our dataset
*   Step 5: Finding out the correlation and visualizing it. We find that air particles have a positive correlation with dew point and snow, while all other variables have a negative correlation  with PM2.5.

 
"""

# Loading our data and setting a consolidated datetime index in pandas

def parse(x):
  return datetime.strptime(x, '%Y %m %d %H')
df = read_csv('pollution.csv', parse_dates=[['year', 'month', 'day', 'hour']], index_col= 0, date_parser=parse)
df.drop('No', axis= 1, inplace= True)

# Old column names

df.columns

# Assigning new column names

df.columns = ['PM2.5_Concentration', 'Dew_Point', 'Temperature', 'Pressure', 'Wind_Direction', 'Wind_Speed', 'Snow', 'Rain']
df.index.name = 'Date'

# Replacing NaN values with 0

df['PM2.5_Concentration'].fillna(0, inplace= True)

# Dropping the first 24 rows

df = df[24:]

# Viewing are DataFrame

df.head()

# Saving our dataset to file

df.to_csv('pollution.csv')

# Finding out the correlation between the pollution levels and input variables

df.corr()

# Visualizing the correlation

df.corr().style.background_gradient(cmap='coolwarm')

# Visualizing our correlation with the use of a heatmap

plt.figure(figsize=(8,8))
seaborn.heatmap(df.corr(), annot=True, cmap='coolwarm')

"""## **Plotting Data**



---

Plotting our data to detect trends and outliers
"""

# Plotting the pollution levels

df.plot(y='PM2.5_Concentration', kind='line', use_index= True, figsize=(10,1))

# Plotting the Temperature levels

df.plot(y='Temperature', kind='line', use_index= True, figsize=(10,1))

# Plotting the Snow levels

df.plot(y='Snow', kind='line', use_index= True, figsize=(10,1))

# Plotting the rain levels

df.plot(y='Rain', kind='line', use_index= True, figsize=(10,1))

"""## **LSTM Data Preperation**



---



*   Step 1: Framing our dataset as a supervised learning problem by predicting the pollution levels at the current hour (t) given the pollution measurement and weather conditions at the previous time step.
*   Step 2: The Wind Direction feature will be label encoded (integer) since it is categorical.
*   Step 3: Normalize the input variables for efficient learning. This is an important step to make sure our model performs efficiently.
*   Step 4: Transforming our dataset into a supervised learning problem
*   Step 5: Removing the weather variables for the hour to be predicted(t)
*   Step 6: We print the transformed dataset and see the 8 input variables and output variable which is the pollution at the current hour.


"""

# Converting series to supervised learning

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg
 
# Load dataset
dataset = read_csv('pollution.csv', header=0, index_col=0)
values = dataset.values

# Label Encode Wind Direction
encoder = LabelEncoder()
values[:,4] = encoder.fit_transform(values[:,4])

# Make sure all data is float
values = values.astype('float32')

# Normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)

# Frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)

# Drop columns we don't want to predict
reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)
print(reframed.head())

"""## **Define And Fit Model**


---



*   Step 1: We will firstly split our training and test sets on a 20 | 80 rule of thumb. This is required in order to first train our dataset and then predict the test set to see if our model is working well.
*   Step 2: Next, we split our train and test sets into input and output variables.
*   Step 3: The inputs(X) are then reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features]. The first dimension represents the batchsize, and the second dimension represents the number of timesteps you are feeding a sequence. The third dimension represents the number of units in one sequence.  
*   Step 4: We design the network and in this case study, we present the best found case where we defined the LSTM with 50 neurons in the first hidden layer and 1 neuron in the outer layer for predicting the pollution levels. The input step was 1 time step with 8 features. It is pertinent to note that by adding another layer, although we can possibly learn more complex relationships, we risk a case of overfitting.
*   Step 5:  For the best found case the model is fit at 50 epochs with a batch size of 72. Although a batch size of 50 also gave decent results. We are also keeping track of training and test loss by setting a validation_data argument.
*   Step 6: We will plot our training and test loss. We can see that the loss has stablized well and both training and test loss are approaching the same levels. From this we can conclude that the model is neither underfitting nor is it overfitting.

"""

# Split into train and test sets
values = reframed.values
n_train_hours = 365 * 24
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]

# Split into input and outputs
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]

# Reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

# Design network
model = Sequential()
model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')

# Fit network
history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)

# Plot history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

"""## **Evaluate Model**



---




*   Step 1: Once the model is fit, we will forecast the test dataset. We will invert the scaling that we did previously using the MinMaxScaler to revert back to the real numbers. We will invert for both the predicted and the forcasted data.
*   Step 2: Once both the predicted and actual values are back in scale, we can calculate the error score for the model. We calculated the RMSE (Root Mean Square Error) that gave us the error in the same units as the variable. We can see that we managed to score a good number at 26.952.



"""

# Make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))

# Invert scaling for forecast
inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]

# Invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]

# Calculate RMSE
rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)

# Printing out the Predicted vs Actual values for a brief comparison
print('Predicted: ', inv_yhat)
print(' ')
print('Actual: ', inv_y)

"""# **Conclusion**


---

In conclusion, our study is merely a simple exploration of LSTM's for time series predictions. There is much more that we can do by diving deeper into its architecture. We can tune the model better and explore other solutions like manipulating the batch size, increasing or decreasing the epoche, train our model on multiple timesteps when calling series_to_supervised(). We can change the number of hours to see what difference it makes. 

Although independently, we tuned the model but the case shown in the study is best suited for presentation purposes. We can see in our figure that the test loss drops below the training loss. This may be due to overfitting so it is advisable to tune the model better. As mentioned before, this case has been presented in such a manner for study purposes.

We have, however, managed to achieve minimal loss and our RMSE score is also respectable.

## **Declaration**



---


This dissertation is a product of my own work and is not the result of anything done in collaboration.
I consent to the University’s free use including online reproduction, including electronically, and including adaptation for teaching and education activities of any whole or part item of this dissertation.

Signed: Syed Salman Raza Naqvi
"""